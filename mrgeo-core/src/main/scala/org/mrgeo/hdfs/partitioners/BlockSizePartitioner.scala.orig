package org.mrgeo.hdfs.partitioners

import java.io.{Externalizable, ObjectInput, ObjectOutput}

import org.apache.hadoop.fs.Path
import org.mrgeo.data.raster.{RasterUtils, RasterWritable}
import org.mrgeo.data.rdd.RasterRDD
import org.mrgeo.hdfs.utils.HadoopFileUtils


class BlockSizePartitioner() extends FileSplitPartitioner() with Externalizable {

  var partitions:Int = 0

  override def numPartitions: Int = { partitions }

  def getPartition(key: Any): Int = 0

  override def readExternal(in: ObjectInput): Unit = {}
  override def writeExternal(out: ObjectOutput): Unit = {}

  def hasFixedPartitions:Boolean = true

  override def calculateNumPartitions(raster:RasterRDD, output:String):Int = {
    val path = new Path(output)
    val fs = HadoopFileUtils.getFileSystem(path)
    val blocksize = fs.getDefaultBlockSize(path)

    val tile = RasterWritable.toRaster(raster.first()._2)

    val pixelbytes = RasterUtils.getElementSize(tile.getSampleModel.getDataType) * tile.getNumBands
    val imagebytes = pixelbytes * tile.getWidth * tile.getHeight

    val tilesperblock = (blocksize / imagebytes) - 1  // subtract 1 for the 0-based counting

<<<<<<< HEAD
    val length = in.readInt()

    var i: Int = 0
    while (i < length) {
      splitsbuilder += in.readLong()
      i += 1
    }
    splits = splitsbuilder.result()
  }
=======
    partitions = Math.ceil(raster.count() / tilesperblock.toDouble).toInt
>>>>>>> master

    partitions
  }
}
